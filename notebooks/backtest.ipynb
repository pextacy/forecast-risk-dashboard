{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treasury Risk Dashboard - Comprehensive Backtesting\n",
    "\n",
    "This notebook provides comprehensive backtesting capabilities for the Treasury Risk Dashboard, validating forecast accuracy and risk management strategies using real historical data.\n",
    "\n",
    "## Contents\n",
    "1. **Setup and Data Loading**\n",
    "2. **Forecast Model Backtesting**\n",
    "3. **Risk Metrics Validation**\n",
    "4. **Hedge Strategy Performance**\n",
    "5. **Model Comparison and Analysis**\n",
    "6. **Performance Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "import asyncio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add backend to path\n",
    "sys.path.append('../backend')\n",
    "\n",
    "# Import our services\n",
    "from app.services.backtesting import forecast_backtester, risk_backtester, performance_analyzer\n",
    "from app.services.ingestion import ingestion_service\n",
    "from app.services.forecasting import forecasting_service\n",
    "from app.services.risk_metrics import risk_calculator\n",
    "from app.db.connection import db_manager\n",
    "from app.utils.config import settings\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Successfully imported all dependencies\")\n",
    "print(f\"üìä Supported crypto assets: {len(settings.supported_crypto_symbols)}\")\n",
    "print(f\"üìà Supported stock assets: {len(settings.supported_stock_symbols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Validation\n",
    "\n",
    "First, let's validate our data availability and setup the backtesting environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for backtesting\n",
    "BACKTEST_SYMBOLS = ['bitcoin', 'ethereum', 'BTC', 'ETH']  # Mix of crypto and traditional assets\n",
    "BACKTEST_START = datetime.now() - timedelta(days=180)  # 6 months back\n",
    "BACKTEST_END = datetime.now() - timedelta(days=30)     # Stop 30 days ago to validate recent predictions\n",
    "FORECAST_HORIZON = 7  # 7-day forecasts\n",
    "\n",
    "print(f\"üóìÔ∏è Backtest Period: {BACKTEST_START.date()} to {BACKTEST_END.date()}\")\n",
    "print(f\"üîÆ Forecast Horizon: {FORECAST_HORIZON} days\")\n",
    "print(f\"üìä Testing Symbols: {BACKTEST_SYMBOLS}\")\n",
    "\n",
    "# Check data availability\n",
    "async def check_data_availability():\n",
    "    print(\"\\nüîç Checking data availability...\")\n",
    "    \n",
    "    for symbol in BACKTEST_SYMBOLS:\n",
    "        try:\n",
    "            history = await db_manager.get_price_history(symbol, 200)\n",
    "            print(f\"  {symbol}: {len(history)} data points available\")\n",
    "            \n",
    "            if len(history) < 90:\n",
    "                print(f\"  ‚ö†Ô∏è Insufficient data for {symbol}, attempting to fetch more...\")\n",
    "                await ingestion_service.ingest_historical_data(symbol, days=200)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Error checking {symbol}: {e}\")\n",
    "\n",
    "# Run data availability check\n",
    "await check_data_availability()\n",
    "print(\"\\n‚úÖ Data availability check completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Forecast Model Backtesting\n",
    "\n",
    "Test the accuracy of our forecasting models using walk-forward validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive forecast backtesting\n",
    "async def run_forecast_backtests():\n",
    "    print(\"üöÄ Starting comprehensive forecast backtesting...\\n\")\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    for symbol in BACKTEST_SYMBOLS:\n",
    "        print(f\"üìà Backtesting forecasts for {symbol}\")\n",
    "        \n",
    "        try:\n",
    "            results = await forecast_backtester.run_forecast_backtest(\n",
    "                symbol=symbol,\n",
    "                start_date=BACKTEST_START,\n",
    "                end_date=BACKTEST_END,\n",
    "                forecast_horizon=FORECAST_HORIZON,\n",
    "                refit_frequency=30,\n",
    "                models=['arima', 'prophet']\n",
    "            )\n",
    "            \n",
    "            all_results[symbol] = results\n",
    "            \n",
    "            # Print summary for this symbol\n",
    "            for model_name, result in results.items():\n",
    "                metrics = result.metrics\n",
    "                print(f\"  {model_name.upper()}:\")\n",
    "                print(f\"    üìä Predictions: {metrics.get('total_predictions', 0)}\")\n",
    "                print(f\"    üìâ MAE: {metrics.get('mae', 0):.2f}\")\n",
    "                print(f\"    üìà MAPE: {metrics.get('mape', 0):.1f}%\")\n",
    "                print(f\"    üéØ Directional Accuracy: {metrics.get('directional_accuracy', 0):.1%}\")\n",
    "                print(f\"    üîó Correlation: {metrics.get('correlation', 0):.3f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Backtest failed for {symbol}: {e}\")\n",
    "            all_results[symbol] = {}\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Run the backtests\n",
    "forecast_results = await run_forecast_backtests()\n",
    "print(\"‚úÖ Forecast backtesting completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Performance Analysis and Visualization\n",
    "\n",
    "Analyze the backtest results and create comprehensive visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze forecast performance\n",
    "def create_performance_summary(results):\n",
    "    \"\"\"Create comprehensive performance summary.\"\"\"\n",
    "    \n",
    "    summary_data = []\n",
    "    \n",
    "    for symbol, models in results.items():\n",
    "        for model_name, result in models.items():\n",
    "            if hasattr(result, 'metrics') and result.metrics:\n",
    "                summary_data.append({\n",
    "                    'Symbol': symbol,\n",
    "                    'Model': model_name.upper(),\n",
    "                    'Predictions': result.metrics.get('total_predictions', 0),\n",
    "                    'MAE': result.metrics.get('mae', 0),\n",
    "                    'RMSE': result.metrics.get('rmse', 0),\n",
    "                    'MAPE (%)': result.metrics.get('mape', 0),\n",
    "                    'Directional Accuracy': result.metrics.get('directional_accuracy', 0),\n",
    "                    'Correlation': result.metrics.get('correlation', 0)\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(summary_data)\n",
    "\n",
    "# Create performance summary\n",
    "summary_df = create_performance_summary(forecast_results)\n",
    "\n",
    "if not summary_df.empty:\n",
    "    print(\"üìä FORECAST PERFORMANCE SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Display formatted summary\n",
    "    pd.set_option('display.float_format', '{:.3f}'.format)\n",
    "    print(summary_df.to_string(index=False))\n",
    "    \n",
    "    # Calculate overall statistics\n",
    "    print(\"\\nüìà OVERALL STATISTICS\")\n",
    "    print(\"=\" * 30)\n",
    "    print(f\"Average MAPE: {summary_df['MAPE (%)'].mean():.1f}%\")\n",
    "    print(f\"Average Directional Accuracy: {summary_df['Directional Accuracy'].mean():.1%}\")\n",
    "    print(f\"Average Correlation: {summary_df['Correlation'].mean():.3f}\")\n",
    "    print(f\"Total Predictions: {summary_df['Predictions'].sum()}\")\n",
    "    \n",
    "    # Find best performing models\n",
    "    best_mape = summary_df.loc[summary_df['MAPE (%)'].idxmin()]\n",
    "    best_correlation = summary_df.loc[summary_df['Correlation'].idxmax()]\n",
    "    \n",
    "    print(f\"\\nüèÜ Best MAPE: {best_mape['Model']} on {best_mape['Symbol']} ({best_mape['MAPE (%)']:.1f}%)\")\n",
    "    print(f\"üèÜ Best Correlation: {best_correlation['Model']} on {best_correlation['Symbol']} ({best_correlation['Correlation']:.3f})\")\n",
    "else:\n",
    "    print(\"‚ùå No forecast results available for analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "if not summary_df.empty:\n",
    "    # Set up the plotting area\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Forecast Model Performance Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. MAPE by Model and Symbol\n",
    "    summary_pivot = summary_df.pivot(index='Symbol', columns='Model', values='MAPE (%)')\n",
    "    sns.heatmap(summary_pivot, annot=True, fmt='.1f', cmap='RdYlGn_r', ax=axes[0,0])\n",
    "    axes[0,0].set_title('MAPE (%) by Model and Symbol\\n(Lower is Better)')\n",
    "    \n",
    "    # 2. Directional Accuracy\n",
    "    summary_df.groupby('Model')['Directional Accuracy'].mean().plot(kind='bar', ax=axes[0,1], color=['skyblue', 'lightcoral'])\n",
    "    axes[0,1].set_title('Average Directional Accuracy by Model')\n",
    "    axes[0,1].set_ylabel('Directional Accuracy')\n",
    "    axes[0,1].set_ylim(0, 1)\n",
    "    axes[0,1].tick_params(axis='x', rotation=0)\n",
    "    \n",
    "    # 3. Correlation Analysis\n",
    "    summary_df.boxplot(column='Correlation', by='Model', ax=axes[1,0])\n",
    "    axes[1,0].set_title('Correlation Distribution by Model')\n",
    "    axes[1,0].set_xlabel('Model')\n",
    "    \n",
    "    # 4. Model Performance Scatter\n",
    "    for model in summary_df['Model'].unique():\n",
    "        model_data = summary_df[summary_df['Model'] == model]\n",
    "        axes[1,1].scatter(model_data['MAPE (%)'], model_data['Correlation'], \n",
    "                         label=model, alpha=0.7, s=100)\n",
    "    \n",
    "    axes[1,1].set_xlabel('MAPE (%)')\n",
    "    axes[1,1].set_ylabel('Correlation')\n",
    "    axes[1,1].set_title('MAPE vs Correlation by Model')\n",
    "    axes[1,1].legend()\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Performance grades analysis\n",
    "    print(\"\\nüéì MODEL PERFORMANCE GRADES\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    for symbol, models in forecast_results.items():\n",
    "        if models:\n",
    "            print(f\"\\n{symbol.upper()}:\")\n",
    "            for model_name, result in models.items():\n",
    "                if hasattr(result, 'metrics'):\n",
    "                    grade = performance_analyzer._grade_performance(result.metrics)\n",
    "                    print(f\"  {model_name.upper()}: Grade {grade}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No data available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Risk Metrics Validation\n",
    "\n",
    "Validate our risk calculation accuracy using historical portfolio data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test portfolio risk calculation accuracy\n",
    "async def validate_risk_metrics():\n",
    "    print(\"üîç Validating portfolio risk metrics...\\n\")\n",
    "    \n",
    "    # Create test portfolios\n",
    "    test_portfolios = {\n",
    "        'Conservative': {'bitcoin': 0.3, 'ethereum': 0.2, 'USDC': 0.5},\n",
    "        'Balanced': {'bitcoin': 0.4, 'ethereum': 0.3, 'USDC': 0.3},\n",
    "        'Aggressive': {'bitcoin': 0.6, 'ethereum': 0.4}\n",
    "    }\n",
    "    \n",
    "    risk_validation_results = {}\n",
    "    \n",
    "    for portfolio_name, weights in test_portfolios.items():\n",
    "        print(f\"üìä Testing {portfolio_name} Portfolio\")\n",
    "        print(f\"   Allocation: {weights}\")\n",
    "        \n",
    "        try:\n",
    "            # Calculate risk metrics\n",
    "            risk_result = await risk_calculator.calculate_portfolio_metrics(\n",
    "                portfolio_weights=weights,\n",
    "                lookback_days=90,\n",
    "                confidence_levels=[0.95, 0.99]\n",
    "            )\n",
    "            \n",
    "            metrics = risk_result['metrics']\n",
    "            \n",
    "            print(f\"   üìà Annual Volatility: {metrics.get('annualized_volatility', 0)*100:.1f}%\")\n",
    "            print(f\"   üìâ VaR (95%): {abs(metrics.get('var_95', {}).get('historical', 0))*100:.1f}%\")\n",
    "            print(f\"   ‚ö° Sharpe Ratio: {metrics.get('sharpe_ratio', 0):.2f}\")\n",
    "            print(f\"   üìä Max Drawdown: {abs(metrics.get('max_drawdown', 0))*100:.1f}%\")\n",
    "            print(f\"   üéØ Concentration Ratio: {metrics.get('concentration_ratio', 0)*100:.1f}%\")\n",
    "            \n",
    "            risk_validation_results[portfolio_name] = metrics\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Risk calculation failed: {e}\")\n",
    "            risk_validation_results[portfolio_name] = {}\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    return risk_validation_results\n",
    "\n",
    "# Run risk validation\n",
    "risk_results = await validate_risk_metrics()\n",
    "print(\"‚úÖ Risk metrics validation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize risk metrics comparison\n",
    "if risk_results:\n",
    "    # Prepare data for visualization\n",
    "    risk_comparison = []\n",
    "    \n",
    "    for portfolio_name, metrics in risk_results.items():\n",
    "        if metrics:\n",
    "            risk_comparison.append({\n",
    "                'Portfolio': portfolio_name,\n",
    "                'Volatility (%)': metrics.get('annualized_volatility', 0) * 100,\n",
    "                'VaR 95% (%)': abs(metrics.get('var_95', {}).get('historical', 0)) * 100,\n",
    "                'Sharpe Ratio': metrics.get('sharpe_ratio', 0),\n",
    "                'Max Drawdown (%)': abs(metrics.get('max_drawdown', 0)) * 100,\n",
    "                'Concentration (%)': metrics.get('concentration_ratio', 0) * 100\n",
    "            })\n",
    "    \n",
    "    if risk_comparison:\n",
    "        risk_df = pd.DataFrame(risk_comparison)\n",
    "        \n",
    "        # Create risk comparison visualization\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        fig.suptitle('Portfolio Risk Metrics Comparison', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # 1. Volatility comparison\n",
    "        risk_df.set_index('Portfolio')['Volatility (%)'].plot(kind='bar', ax=axes[0,0], color='skyblue')\n",
    "        axes[0,0].set_title('Annual Volatility by Portfolio')\n",
    "        axes[0,0].set_ylabel('Volatility (%)')\n",
    "        axes[0,0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # 2. VaR comparison\n",
    "        risk_df.set_index('Portfolio')['VaR 95% (%)'].plot(kind='bar', ax=axes[0,1], color='lightcoral')\n",
    "        axes[0,1].set_title('Value at Risk (95%) by Portfolio')\n",
    "        axes[0,1].set_ylabel('VaR (%)')\n",
    "        axes[0,1].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # 3. Risk-Return scatter\n",
    "        for i, row in risk_df.iterrows():\n",
    "            axes[1,0].scatter(row['Volatility (%)'], row['Sharpe Ratio'], \n",
    "                            label=row['Portfolio'], s=150, alpha=0.7)\n",
    "        \n",
    "        axes[1,0].set_xlabel('Volatility (%)')\n",
    "        axes[1,0].set_ylabel('Sharpe Ratio')\n",
    "        axes[1,0].set_title('Risk-Return Profile')\n",
    "        axes[1,0].legend()\n",
    "        axes[1,0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. Risk metrics radar chart\n",
    "        metrics_to_plot = ['Volatility (%)', 'VaR 95% (%)', 'Max Drawdown (%)', 'Concentration (%)']\n",
    "        \n",
    "        for portfolio in risk_df['Portfolio']:\n",
    "            portfolio_data = risk_df[risk_df['Portfolio'] == portfolio]\n",
    "            values = [portfolio_data[metric].iloc[0] for metric in metrics_to_plot]\n",
    "            axes[1,1].plot(metrics_to_plot, values, marker='o', label=portfolio, linewidth=2)\n",
    "        \n",
    "        axes[1,1].set_title('Risk Metrics Profile')\n",
    "        axes[1,1].legend()\n",
    "        axes[1,1].tick_params(axis='x', rotation=45)\n",
    "        axes[1,1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print risk assessment\n",
    "        print(\"\\nüéØ RISK ASSESSMENT SUMMARY\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        for _, row in risk_df.iterrows():\n",
    "            portfolio = row['Portfolio']\n",
    "            volatility = row['Volatility (%)']\n",
    "            var = row['VaR 95% (%)']\n",
    "            sharpe = row['Sharpe Ratio']\n",
    "            \n",
    "            risk_level = 'Low' if volatility < 20 else 'Medium' if volatility < 40 else 'High'\n",
    "            return_quality = 'Excellent' if sharpe > 1 else 'Good' if sharpe > 0.5 else 'Poor'\n",
    "            \n",
    "            print(f\"\\n{portfolio} Portfolio:\")\n",
    "            print(f\"  Risk Level: {risk_level} (Vol: {volatility:.1f}%)\")\n",
    "            print(f\"  Return Quality: {return_quality} (Sharpe: {sharpe:.2f})\")\n",
    "            print(f\"  Potential Daily Loss: {var:.1f}% (95% confidence)\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No risk data available for comparison\")\n",
    "else:\n",
    "    print(\"‚ùå No risk validation results available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Real-Time Forecast Accuracy\n",
    "\n",
    "Test the accuracy of recent forecasts against actual market movements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test recent forecast accuracy\n",
    "async def test_recent_forecasts():\n",
    "    print(\"üéØ Testing recent forecast accuracy...\\n\")\n",
    "    \n",
    "    recent_accuracy = []\n",
    "    \n",
    "    for symbol in ['bitcoin', 'ethereum']:  # Focus on crypto for real-time testing\n",
    "        try:\n",
    "            print(f\"üìä Analyzing recent forecasts for {symbol}\")\n",
    "            \n",
    "            # Get forecast accuracy for last 30 days\n",
    "            accuracy_result = await forecasting_service.get_forecast_accuracy(symbol, 30)\n",
    "            \n",
    "            if 'error' not in accuracy_result and accuracy_result:\n",
    "                # Calculate average accuracy metrics\n",
    "                all_mae = []\n",
    "                all_mape = []\n",
    "                \n",
    "                for forecast_key, metrics in accuracy_result.items():\n",
    "                    if isinstance(metrics, dict):\n",
    "                        if 'mae' in metrics:\n",
    "                            all_mae.append(metrics['mae'])\n",
    "                        if 'mape' in metrics:\n",
    "                            all_mape.append(metrics['mape'])\n",
    "                \n",
    "                if all_mae and all_mape:\n",
    "                    avg_mae = np.mean(all_mae)\n",
    "                    avg_mape = np.mean(all_mape)\n",
    "                    \n",
    "                    recent_accuracy.append({\n",
    "                        'Symbol': symbol,\n",
    "                        'Forecasts_Analyzed': len(all_mae),\n",
    "                        'Average_MAE': avg_mae,\n",
    "                        'Average_MAPE': avg_mape\n",
    "                    })\n",
    "                    \n",
    "                    print(f\"  ‚úÖ {len(all_mae)} recent forecasts analyzed\")\n",
    "                    print(f\"     Average MAE: {avg_mae:.2f}\")\n",
    "                    print(f\"     Average MAPE: {avg_mape:.1f}%\")\n",
    "                else:\n",
    "                    print(f\"  ‚ö†Ô∏è No valid accuracy metrics found\")\n",
    "            else:\n",
    "                print(f\"  ‚ö†Ô∏è No recent forecasts available or error: {accuracy_result.get('error', 'Unknown')}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Error analyzing {symbol}: {e}\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    return recent_accuracy\n",
    "\n",
    "# Test recent accuracy\n",
    "recent_results = await test_recent_forecasts()\n",
    "\n",
    "if recent_results:\n",
    "    recent_df = pd.DataFrame(recent_results)\n",
    "    print(\"üìà RECENT FORECAST ACCURACY SUMMARY\")\n",
    "    print(\"=\" * 45)\n",
    "    print(recent_df.to_string(index=False, float_format='%.2f'))\n",
    "    \n",
    "    # Overall assessment\n",
    "    if len(recent_results) > 0:\n",
    "        avg_mape = np.mean([r['Average_MAPE'] for r in recent_results])\n",
    "        total_forecasts = sum([r['Forecasts_Analyzed'] for r in recent_results])\n",
    "        \n",
    "        print(f\"\\nüéØ Overall Recent Performance:\")\n",
    "        print(f\"   Total Forecasts: {total_forecasts}\")\n",
    "        print(f\"   Average MAPE: {avg_mape:.1f}%\")\n",
    "        \n",
    "        if avg_mape < 10:\n",
    "            print(f\"   üèÜ Excellent forecast accuracy!\")\n",
    "        elif avg_mape < 20:\n",
    "            print(f\"   ‚úÖ Good forecast accuracy\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è Forecast accuracy needs improvement\")\nelse:\n",
    "    print(\"‚ÑπÔ∏è No recent forecast data available for accuracy testing\")\n",
    "\n",
    "print(\"\\n‚úÖ Recent forecast accuracy testing completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comprehensive Summary and Recommendations\n",
    "\n",
    "Generate final assessment and recommendations based on all backtesting results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive summary\n",
    "print(\"üìã COMPREHENSIVE BACKTESTING SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Overall system assessment\n",
    "print(\"\\nüéØ SYSTEM PERFORMANCE ASSESSMENT\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "assessments = []\n",
    "\n",
    "# Forecast accuracy assessment\n",
    "if not summary_df.empty:\n",
    "    avg_mape = summary_df['MAPE (%)'].mean()\n",
    "    avg_directional = summary_df['Directional Accuracy'].mean()\n",
    "    \n",
    "    forecast_grade = 'A' if avg_mape < 10 else 'B' if avg_mape < 15 else 'C' if avg_mape < 25 else 'D'\n",
    "    assessments.append(f\"üìà Forecast Accuracy: Grade {forecast_grade} (MAPE: {avg_mape:.1f}%, Dir: {avg_directional:.1%})\")\n",
    "    \n",
    "    if avg_directional > 0.6:\n",
    "        assessments.append(\"‚úÖ Strong trend prediction capability\")\n",
    "    else:\n",
    "        assessments.append(\"‚ö†Ô∏è Trend prediction needs improvement\")\nelse:\n",
    "    assessments.append(\"‚ùå Forecast accuracy assessment unavailable\")\n",
    "\n",
    "# Risk calculation assessment\n",
    "if risk_results:\n",
    "    risk_calculated = len([r for r in risk_results.values() if r])\n",
    "    assessments.append(f\"üõ°Ô∏è Risk Calculations: {risk_calculated}/{len(risk_results)} portfolios successfully analyzed\")\n",
    "    \n",
    "    # Check risk differentiation\n",
    "    volatilities = [r.get('annualized_volatility', 0) for r in risk_results.values() if r]\n",
    "    if len(volatilities) > 1 and max(volatilities) > min(volatilities) * 1.5:\n",
    "        assessments.append(\"‚úÖ Risk metrics properly differentiate portfolio risk levels\")\n",
    "    else:\n",
    "        assessments.append(\"‚ö†Ô∏è Risk differentiation may need calibration\")\nelse:\n",
    "    assessments.append(\"‚ùå Risk calculation assessment unavailable\")\n",
    "\n",
    "# Recent performance assessment\n",
    "if recent_results:\n",
    "    recent_mape = np.mean([r['Average_MAPE'] for r in recent_results])\n",
    "    assessments.append(f\"üîÑ Recent Performance: {recent_mape:.1f}% MAPE on latest forecasts\")\n",
    "    \n",
    "    if recent_mape < 15:\n",
    "        assessments.append(\"‚úÖ Current model performance is stable\")\n",
    "    else:\n",
    "        assessments.append(\"‚ö†Ô∏è Recent performance degradation detected\")\nelse:\n",
    "    assessments.append(\"‚ÑπÔ∏è Recent performance data not available\")\n",
    "\n",
    "# Print assessments\n",
    "for assessment in assessments:\n",
    "    print(assessment)\n",
    "\n",
    "# Recommendations\n",
    "print(\"\\nüí° RECOMMENDATIONS\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "recommendations = []\n",
    "\n",
    "# Model-specific recommendations\n",
    "if not summary_df.empty:\n",
    "    best_model = summary_df.loc[summary_df['MAPE (%)'].idxmin(), 'Model']\n",
    "    worst_model = summary_df.loc[summary_df['MAPE (%)'].idxmax(), 'Model']\n",
    "    \n",
    "    recommendations.append(f\"üèÜ Prioritize {best_model} model for production forecasts\")\n",
    "    \n",
    "    if summary_df['MAPE (%)'].max() > 25:\n",
    "        recommendations.append(f\"üîß Consider retuning or replacing {worst_model} model\")\n",
    "    \n",
    "    # Data quality recommendations\n",
    "    total_predictions = summary_df['Predictions'].sum()\n",
    "    if total_predictions < 50:\n",
    "        recommendations.append(\"üìä Increase historical data collection for more robust backtesting\")\n",
    "    \n",
    "    # Ensemble recommendations\n",
    "    if len(summary_df['Model'].unique()) > 1:\n",
    "        recommendations.append(\"üîÄ Consider ensemble forecasting to combine model strengths\")\n",
    "\n",
    "# Risk management recommendations\n",
    "if risk_results:\n",
    "    high_vol_portfolios = [name for name, metrics in risk_results.items() \n",
    "                          if metrics and metrics.get('annualized_volatility', 0) > 0.4]\n",
    "    \n",
    "    if high_vol_portfolios:\n",
    "        recommendations.append(f\"‚ö†Ô∏è Monitor high-volatility portfolios: {', '.join(high_vol_portfolios)}\")\n",
    "    \n",
    "    recommendations.append(\"üõ°Ô∏è Implement automated risk alerting based on VaR thresholds\")\n",
    "\n",
    "# Operational recommendations\n",
    "recommendations.extend([\n",
    "    \"üîÑ Run backtesting monthly to monitor model performance\",\n",
    "    \"üìà Implement walk-forward validation for production forecasts\",\n",
    "    \"üéØ Set MAPE targets: <10% excellent, <15% good, >20% needs improvement\",\n",
    "    \"‚è∞ Consider real-time model performance monitoring\"\n",
    "])\n",
    "\n",
    "# Print recommendations\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"{i:2d}. {rec}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"üéâ BACKTESTING ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Save results summary\n",
    "print(\"\\nüíæ Saving results summary...\")\n",
    "\n",
    "# Create results summary\n",
    "results_summary = {\n",
    "    'backtest_date': datetime.now().isoformat(),\n",
    "    'backtest_period': f\"{BACKTEST_START.date()} to {BACKTEST_END.date()}\",\n",
    "    'symbols_tested': BACKTEST_SYMBOLS,\n",
    "    'forecast_horizon': FORECAST_HORIZON,\n",
    "    'forecast_performance': summary_df.to_dict('records') if not summary_df.empty else [],\n",
    "    'risk_validation': risk_results,\n",
    "    'recent_accuracy': recent_results,\n",
    "    'assessments': assessments,\n",
    "    'recommendations': recommendations\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "import json\n",
    "\n",
    "with open('../data/backtest_results.json', 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2, default=str)\n",
    "\n",
    "print(\"‚úÖ Results saved to ../data/backtest_results.json\")\n",
    "print(\"\\nüöÄ Treasury Risk Dashboard backtesting complete!\")\n",
    "print(\"   Ready for production deployment with validated models.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}